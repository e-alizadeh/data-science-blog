{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Guide to Metrics (Estimates) in Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "# display.Image(\"https://www.ealizadeh.com/wp-content/uploads/2020/11/BP05_featured_image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Website: https://ealizadeh.com**\n",
    "\n",
    "**Medium: https://medium.com/@ealizadeh**\n",
    "\n",
    "Copyright Â© 2020 Esmaeil Alizadeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis (EDA) is an important step in any data science project. We always try to get a glance of our data by computing descriptive statistics of our dataset. If you are like me, the first function you call might be Pandas dataframe.describe() to obtain descriptive statistics. While such analysis is important, we often underestimate the importance of choosing the correct sample statistics/metrics/estimates.\n",
    "\n",
    "In this post, we will go over several metrics that you can use in your data science projects. In particular, we are going to cover several estimates of location and variability and their robustness (sensitiveness to outliers).\n",
    "\n",
    "The following common metrics/estimates are covered in this article:\n",
    "- Estimates of location (first moment of the distribution)\n",
    "   - mean, trimmed/truncated mean, weighted mean\n",
    "   - median, weighted median\n",
    "- Estimates of variability (second moment of the distribution)\n",
    "   - range\n",
    "   - variance and standard deviation\n",
    "   - mean absolute deviation, median absolute deviation\n",
    "   - percentiles (quantiles)\n",
    "   \n",
    "For each metric, we will cover:\n",
    "- The definition and mathematical formulation along with some insights.\n",
    "- Whether the metric is robust (sensitiveness to extreme cases)\n",
    "- Python implementation and an example\n",
    "\n",
    "Note: The focus of this article is on the metrics and estimates used in the univariate analysis of numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimates of Location\n",
    "\n",
    "Estimates of location are measures of the central tendency of the data (where most of the data is located). In statistics, this is usually referred to as the first moment of a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation\n",
    "\n",
    "You can use NumPy's average() function to calculate the mean and weighted mean (equations 1.1 & 1.2). For computing truncated mean, you can use trim_mean() from the SciPy stats module. A common choice for truncating the top and bottom of the data is 10%[1].\n",
    "\n",
    "You can use NumPy's median() function to calculate the median. For computing the weighted median, you can use weighted_median() from the robustats Python library (you can install it using pip install robustats). Robustats is a high-performance Python library to compute robust statistical estimators implemented in C.\n",
    "\n",
    "For computing the mode, you can either use the mode() function either from the robustats library that is particularly useful on large datasets or from scipy.stats module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import robustats \n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"data\": [2, 1, 2, 3, 2, 2, 3, 20],\n",
    "    \"weights\": [1, 0.5, 1, 1, 1, 1, 1, 0.5] # Not necessarily add up to 1!!\n",
    "})\n",
    "data, weights = df[\"data\"], df[\"weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  4.375\n",
      "Weighted Mean:  3.5\n",
      "Truncated Mean:  4.375\n",
      "Median:  2.0\n",
      "Weighted Median:  2.0\n",
      "Mode:  ModeResult(mode=array([2]), count=array([4]))\n"
     ]
    }
   ],
   "source": [
    "mean = np.average(data) # You can use Pandas dataframe.mean()\n",
    "weighted_mean = np.average(data, weights=weights)\n",
    "truncated_mean = stats.trim_mean(data, proportiontocut=0.1)\n",
    "median = np.median(data) # You can use Pandas dataframe.median()\n",
    "weighted_median = robustats.weighted_median(x=data, weights=weights)\n",
    "mode = stats.mode(data)  # You can also use robustats.mode() on larger datasets\n",
    "\n",
    "\n",
    "print(\"Mean: \", mean.round(3))\n",
    "print(\"Weighted Mean: \", weighted_mean.round(3))\n",
    "print(\"Truncated Mean: \", truncated_mean.round(3))\n",
    "print(\"Median: \", median)\n",
    "print(\"Weighted Median: \", weighted_median)\n",
    "print(\"Mode: \", mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if we just remove 20 from our data, how that will impact our mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  2.143\n"
     ]
    }
   ],
   "source": [
    "mean = np.average(data[:-1]) # Remove the last data point (20)\n",
    "print(\"Mean: \", mean.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the last data point (20) impacted the mean (4.375 vs 2.143). There can be many situations that we may end up with some outliers that should be cleaned from our datasets like faulty measurements that are in orders of magnitude away from other data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimates of Variability\n",
    "The second dimension (or moment) addresses how the data is spread out (variability or dispersion of the data). For this, we have to measure the difference (aka residual) between an estimate of location and an observed value[1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation\n",
    "\n",
    "You can use NumPy's var() and std() function to calculate the variance and standard deviation, respectively. On the other hand, to calculate the mean absolute deviation, you can use Pandas mad() function. For computing the trimmed standard deviation, you can use SciPy's tstd() from the stats module. You can use Pandas boxplot() to quickly visualize a boxplot of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Variability Estimates of State Population "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance:  35.234\n",
      "Standard Deviation:  5.936\n",
      "Mean Absolute Deviation:  3.906\n",
      "Trimmed Standard Deviation:  6.346\n",
      "Median Absolute Deviation:  0.741\n",
      "Interquantile Range (IQR):  1.0\n"
     ]
    }
   ],
   "source": [
    "variance = np.var(data)\n",
    "standard_deviation = np.std(data)  # df[\"Population\"].std()\n",
    "mean_absolute_deviation = df[\"data\"].mad()\n",
    "trimmed_standard_deviation = stats.tstd(data)\n",
    "median_absolute_deviation = stats.median_abs_deviation(data, scale=\"normal\")  # stats.median_absolute_deviation() is deprecated\n",
    "\n",
    "# Percentile\n",
    "Q1 = np.quantile(data, q=0.25)  # Can also use data.quantile(0.25)\n",
    "Q3 = np.quantile(data, q=0.75)  # Can also use data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(\"Variance: \", variance.round(3))\n",
    "print(\"Standard Deviation: \", standard_deviation.round(3))\n",
    "print(\"Mean Absolute Deviation: \", mean_absolute_deviation.round(3))\n",
    "print(\"Trimmed Standard Deviation: \", trimmed_standard_deviation.round(3))\n",
    "print(\"Median Absolute Deviation: \", median_absolute_deviation.round(3))\n",
    "print(\"Interquantile Range (IQR): \", IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this post, I talked about various estimates of location and variability. In particular, I covered more than 10 different sample statistics and whether they are robust metrics or not. A table of all the metric along with their corresponding Python and R functions are summarized in Table 3. We also saw how the presence of an outlier may impact non-robust metrics like the mean. In this case, we may want to use a robust estimate. However, in some problems, we are interested in studying extreme cases and outliers such as anomaly detection.  \n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] P. Bruce & A. Bruce (2017), *Practical Statistics for Data Scientists*, First Edition, O'Reilly\n",
    "\n",
    "[2] Wikipedia, [Truncated mean](https://en.wikipedia.org/wiki/Truncated_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "[1] Michael Galarnyk (2018), [Understanding Boxplots](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51), Towards Data Science blog"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
